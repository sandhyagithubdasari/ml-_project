{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandhyagithubdasari/sample_eda_template_project/blob/main/RETAIL_SALES_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is a live dataset of Rossmann Stores. On analsysing this problem we observe that Rossmann problem is a regression problem and our primarily goal is to predict the sales figures of Rossmann problem. In this Notebook we work on following topics\n",
        "\n",
        "Analysing the Dataset by using Exploratory Data Analysis. Using Exponential Moving Averages analyse Trends and Seasonality in Roseman dataset. Analyse Regression analysis using following prediction analysis, A. Linear Regression Analysis B. Elastic Regression ( Lasso and Ridge Regression). C. Random Forest Regression. d.adaboost and Xgboost).\n",
        "\n",
        "By applying above algorthim we find accuracy of 98% by Xgboost."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.\n",
        "You are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set. Note that some stores in the dataset were temporarily closed for refurbishment.\n",
        "Data Description"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Description**"
      ],
      "metadata": {
        "id": "cZjxqFSRaIJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rossmann Stores Data.csv - historical data including Sales\n",
        "\n",
        "store.csv - supplemental information about the store\n",
        "\n",
        "Data fields Most of the fields are self-explanatory. The following are descriptions for those that aren't.\n",
        "\n",
        "Id - an Id that represents a (Store, Date) duple within the test set\n",
        "\n",
        "Store - a unique Id for each store\n",
        "\n",
        "Sales - the turnover for any given day (this is what you are predicting)\n",
        "\n",
        "Customers - the number of customers on a given day\n",
        "\n",
        "Open - an indicator for whether the store was open: 0 = closed, 1 = open\n",
        "\n",
        "StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
        "\n",
        "SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n",
        "\n",
        "StoreType - differentiates between 4 different store models: a, b, c, d\n",
        "\n",
        "Assortment - describes an assortment level: a = basic, b = extra, c = extended\n",
        "\n",
        "CompetitionDistance - distance in meters to the nearest competitor store\n",
        "\n",
        "*CompetitionOpenSince[Month/Year] *- gives the approximate year and month of the time the nearest competitor was opened\n",
        "\n",
        "Promo - indicates whether a store is running a promo on that day\n",
        "\n",
        "Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
        "\n",
        "Promo2Since[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
        "\n",
        "PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store\n",
        "\n"
      ],
      "metadata": {
        "id": "xVCXqSUyaXG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjtqjAbGHZeM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import ast\n",
        "import math\n",
        "import random\n",
        "from scipy.stats import skew\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score as r2, mean_squared_error as mse\n",
        "import math\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset : - Rossmann Dataset - https://drive.google.com/file/d/1RLUf6NEpRg6sefWqt-nyqLDyMEHryxrl/view?usp=share_link\n",
        "\n",
        "Store Dataset :-https://drive.google.com/file/d/1Tu5r9A1Izhf0JabF473mmyj3jGW-RqnB/view?usp=sharing"
      ],
      "metadata": {
        "id": "KwAQ4CAUbFL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Database = \"/content/drive/MyDrive/Rossmann Stores Data (1).csv\"\n",
        "sales_df = pd.read_csv(Database)\n",
        "Database = \"/content/drive/MyDrive/store.csv\"\n",
        "store_df =pd.read_csv(Database)"
      ],
      "metadata": {
        "id": "v9RZYnkhbvJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "sales_df.head()"
      ],
      "metadata": {
        "id": "oEnw-fmdbuKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.info()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.tail()"
      ],
      "metadata": {
        "id": "OSaAQi66cveh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.describe()"
      ],
      "metadata": {
        "id": "qojm7LyVdE8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change state holiday value a,b,c is equal to 1\n",
        "sales_df['StateHoliday'].value_counts()"
      ],
      "metadata": {
        "id": "HEzLOYC4dNpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.isnull().sum()"
      ],
      "metadata": {
        "id": "WbYctvE8dcgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change datatype object into date format\n",
        "sales_df['Date'].unique()"
      ],
      "metadata": {
        "id": "bobdEiFddR6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_df.info()"
      ],
      "metadata": {
        "id": "lcjSzCq3eLqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.isnull().sum()"
      ],
      "metadata": {
        "id": "qDRFkJFJeexB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df['SchoolHoliday'] .unique()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import dataclass_transform\n",
        "data = [\"DayOfWeek\" , \"StateHoliday\" , \"SchoolHoliday\"]\n",
        "\n",
        "for i in data:\n",
        "  print(i)\n",
        "  print(sales_df[i].unique())\n",
        "  print(\"-----------------------\")"
      ],
      "metadata": {
        "id": "FF5p0f-J2wrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_df.head()"
      ],
      "metadata": {
        "id": "PB7HKpA-3o57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_df.isnull().sum()"
      ],
      "metadata": {
        "id": "JQY-39X-2yGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_df"
      ],
      "metadata": {
        "id": "x-FBnf2S3Npl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STORE DATASET FILL INTO NULL VALUES I.E 0\n",
        "store_df['CompetitionDistance'] = store_df['CompetitionDistance'].fillna(0)\n",
        "store_df['CompetitionOpenSinceMonth'] = store_df['CompetitionOpenSinceMonth'].fillna(0)\n",
        "store_df['CompetitionOpenSinceYear'] = store_df['CompetitionOpenSinceYear'].fillna(0)\n",
        "store_df['Promo2SinceWeek'] = store_df['Promo2SinceWeek'].fillna(0)\n",
        "store_df['Promo2SinceYear'] = store_df['Promo2SinceYear'].fillna(0)\n",
        "store_df['PromoInterval'] = store_df['PromoInterval'].fillna(0)"
      ],
      "metadata": {
        "id": "mG7Q9jyJ3RF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MERGING THE DATAFRAMES**\n"
      ],
      "metadata": {
        "id": "ZKKJGou64AOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final1 = pd.merge(sales_df, store_df, on='Store', how='left')"
      ],
      "metadata": {
        "id": "-OwqnbAR3et1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final1"
      ],
      "metadata": {
        "id": "AVx3khHj5QaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final1.duplicated().sum()"
      ],
      "metadata": {
        "id": "RpsfOgn86oOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change data types object to int\n",
        "final1.loc[final1['StateHoliday'] == '0', 'StateHoliday'] = 0\n",
        "final1.loc[final1['StateHoliday'] == 'a', 'StateHoliday'] = 1\n",
        "final1.loc[final1['StateHoliday'] == 'b', 'StateHoliday'] = 2\n",
        "final1.loc[final1['StateHoliday'] == 'c', 'StateHoliday'] = 3\n",
        "#store the value with same column name i.e StateHoliday with function astype\n",
        "final1['StateHoliday'] = final1['StateHoliday'].astype(int, copy=False)"
      ],
      "metadata": {
        "id": "FKQCb0rU6wT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final1.loc[final1['StoreType'] == 'a', 'StoreType'] = 0\n",
        "final1.loc[final1['StoreType'] == 'b', 'StoreType'] = 1\n",
        "final1.loc[final1['StoreType'] == 'c', 'StoreType'] = 2\n",
        "final1.loc[final1['StoreType'] == 'd', 'StoreType'] = 3\n",
        "#store the value with same column name i.e Assortment with function astype\n",
        "final1['StoreType'] = final1['StoreType'].astype(int, copy=False)"
      ],
      "metadata": {
        "id": "1fQ0M-FA6376"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change Data Types object into int\n",
        "final1.loc[final1['Assortment'] == 'a', 'Assortment'] = 0\n",
        "final1.loc[final1['Assortment'] == 'b', 'Assortment'] = 1\n",
        "final1.loc[final1['Assortment'] == 'c', 'Assortment'] = 2\n",
        "#store the value with same column name i.e Assortment with function astype\n",
        "final1['Assortment'] = final1['Assortment'].astype(int, copy=False)\n"
      ],
      "metadata": {
        "id": "IQz8wx408zIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final1[['StateHoliday', 'Assortment', 'StoreType']].nunique())"
      ],
      "metadata": {
        "id": "iaswB6mb9ptO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final1.info()"
      ],
      "metadata": {
        "id": "k_yxO4OGBcvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final1[\"Sales\"].value_counts()"
      ],
      "metadata": {
        "id": "2qqNhuBfCsIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final1.head()"
      ],
      "metadata": {
        "id": "4pDkJLduC-5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Sales**"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Line chart\n",
        "plt.figure(figsize=(15,6))\n",
        "sns.pointplot(x= 'CompetitionOpenSinceYear', y= 'Sales', data=final1)\n",
        "plt.title('Plot between Sales and Competition Open Since year')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the Plot I  can tell that Sales are high during the year 1900, as there are very few store were operated of Rossmann so there is less competition and sales are high. But as year pass on number of stores increased that means competition also increased and this leads to decline in the sales."
      ],
      "metadata": {
        "id": "vF5M3CUKGtG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "sns.pointplot(x= 'Promo2SinceYear', y= 'Sales', data=final1,color='Red')\n",
        "sns.set_style(\"dark\")\n",
        "plt.title('Plot between Sales and Promo2SinceYear')\n"
      ],
      "metadata": {
        "id": "vzazwGGYHlIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " From this Plot between Sales and promo2 since year shows that effect of sales of stores which continue their promotion. this data is available from yaer 2009 to 2015. Promo2 has very good effect on sales but in year 2013 sales be minimum and also in year 2012 and 2015 sales are very low."
      ],
      "metadata": {
        "id": "npLGhWfEIGVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "sns.pointplot(x= 'DayOfWeek', y= 'Sales', data=final1)\n",
        "plt.title('Plot between Sales and Day of Week')"
      ],
      "metadata": {
        "id": "h6A0M2uCIXWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot between Sales and Days of week shows that maximum sales is on Monday and sales gradually decreasing to 6th day of week i.e. on Saturday. It also shows that sales on Sunday is almost near to zero as on sunday maximum stores are closed."
      ],
      "metadata": {
        "id": "vcDvXOn9IhBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BoxPlot of sales between Assortment and store type**"
      ],
      "metadata": {
        "id": "IRfvHiahI1LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 11))\n",
        "plot_storetype_sales = sns.boxplot(x=\"StoreType\", y=\"Sales\", data=final1, saturation=1.5,width=0.8)\n",
        "plt.title('Boxplot For Sales Values')"
      ],
      "metadata": {
        "id": "ZQu-SdNIJKM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plot_storetype_sales = sns.boxplot(x=\"Assortment\", y=\"Sales\", data=final1)\n",
        "plt.title('Boxplot For Sales Values on the basis of Assortment Level')"
      ],
      "metadata": {
        "id": "yrwYL8X8Jf6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x= 'DayOfWeek', hue='Open', data= final1, palette='cool')\n",
        "plt.title('Store Daily Open Countplot')"
      ],
      "metadata": {
        "id": "D3Gt2FUzKYHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x= 'DayOfWeek', hue='Promo', data= final1, palette='cool')\n",
        "plt.title('Store Daily Promo Countplot')"
      ],
      "metadata": {
        "id": "_asFhV3JKsxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "promo_sales = sns.barplot(x=\"Promo\", y=\"Sales\", data=final1, palette='viridis')"
      ],
      "metadata": {
        "id": "uScQfQcdK-dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here 0 represents the store which didnt opt for promotion and 1 represents for stores who opt for promotion. Those store who took promotions their sales are high as compared to stores who didnt took promotion."
      ],
      "metadata": {
        "id": "8Qu-6POfLB-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StateHoliday and SchoolHoliday**"
      ],
      "metadata": {
        "id": "ctRpBifiMeDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales during state holiday"
      ],
      "metadata": {
        "id": "PCobts9nMtnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0 = public holiday, 1 = Easter holiday, 2 = Christmas, 3 = None"
      ],
      "metadata": {
        "id": "LRdBzg0wMDVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stateholiday_sales = sns.barplot(x=\"StateHoliday\", y=\"Sales\", data=final1)"
      ],
      "metadata": {
        "id": "7ewK78j7MHXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales during School Holiday"
      ],
      "metadata": {
        "id": "Bc_Aw7PIM4Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schoolholiday_sales = sns.barplot(x=\"SchoolHoliday\", y=\"Sales\", data=final1)"
      ],
      "metadata": {
        "id": "fek_2wBDM1AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can observe that most of the stores remain closed during State and Holidays. But it is interesting to note that the number of stores opened during School Holidays were more than that were opened during State Holidays. Another important thing to note is that the stores which were opened during School holidays had more sales than normal."
      ],
      "metadata": {
        "id": "tTEBrJFyNXIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusions from EDA**\n",
        "\n",
        "  1. There are two datasets - 1) Rossmann.csv & 2) Store.csv\n",
        "  2. There's a positive correlation between customers and sales which is\n",
        "    explanatory.\n",
        "  3. From plot sales and competition Open Since Month shows sales go increasing\n",
        "   from November and highest in month December.\n",
        "\n",
        "  4.From plot Sales and day of week, Sales highest on Monday and start  \n",
        "      declining  from Tuesday to Saturday and on Sunday Sales almost near to Zero.\n",
        "\n",
        "  5.Plot between Promotion and Sales shows that promotion helps in increasing\n",
        "     Sales.\n",
        "\n",
        "  6.Type of Store plays an important role in opening pattern of stores.\n",
        "  \n",
        "  7.We can observe that most of the stores remain closed during State Holidays. But it is interesting to note that the number of stores opened during School Holidays were more than that were opened during State Holidays.\n"
      ],
      "metadata": {
        "id": "5XP3Kn16Nj6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering & Data Pre-processing**"
      ],
      "metadata": {
        "id": "e-lAzYTfXECS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,8))\n",
        "correlation = final1.corr()\n",
        "sns.heatmap(abs(correlation), annot=True, cmap='Reds',linewidths=2,fmt=\".2f\")"
      ],
      "metadata": {
        "id": "vGYznxOdXLg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The color of the cell indicates the direction and strength of the correlation: a positive correlation is indicated by a warm color (such as Red) and a negative correlation is indicated by a cool color (such as Orange).The intensity of the color represents the strength of the correlation."
      ],
      "metadata": {
        "id": "E7YrS-K6Xhq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multicollinearity\n",
        "\n",
        "\n",
        "Multicollinearity is a statistical term that refers to the situation where two or more predictor variables in a regression model are highly correlated with each other."
      ],
      "metadata": {
        "id": "s25tB-_MYHQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "-CxvZoD9Y3bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(final1[[i for i in final1.describe().columns if i not in ['Sales']]])"
      ],
      "metadata": {
        "id": "2KBu_ny8Y_7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In above table we can see that VIF(Variance Inflation Factor) value for column Promo2 and Promo2SinceYear is Higher .So we will drop either Promo2 or Promo2SinceYear and again check VIF value.Here we drop Promo2 column."
      ],
      "metadata": {
        "id": "rfTKAQwoZN0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIF factor below 10 is look good for Machine Learning Model."
      ],
      "metadata": {
        "id": "d9u_RW9ONhti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of Target Variable i.e 'Sales'.**\n",
        "\n"
      ],
      "metadata": {
        "id": "9oeAdYH5NkFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis on Sales - Dependent variable**"
      ],
      "metadata": {
        "id": "zA2xc9duaZR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(final1['Sales'],).hist(bins=5, color=\"yellow\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MV6qNXegbBO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final1[(final1.Open == 0) & (final1.Sales == 0)].count()[0]"
      ],
      "metadata": {
        "id": "Yv2M1sh_bJIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we will drop thoose store which sales is 0 assuming that the stores were closed temoprarily and this will help to train the model more accurately."
      ],
      "metadata": {
        "id": "UU-hgPIoN3vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = final1.drop(final1[(final1.Open == 0) & (final1.Sales == 0)].index)"
      ],
      "metadata": {
        "id": "SLXcax-oa66r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head()"
      ],
      "metadata": {
        "id": "W1sKdxwOb1sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.shape"
      ],
      "metadata": {
        "id": "9mgXJ-myb7b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In new_df dataset,column name 'PromoInterval' change into dummies it means that each new column will have a binary value (0 or 1)."
      ],
      "metadata": {
        "id": "jiCrk3GOGDNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.get_dummies(new_df, columns=['PromoInterval'])"
      ],
      "metadata": {
        "id": "sLP_lC8EGEPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df"
      ],
      "metadata": {
        "id": "GFQpxcW9GQvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis Testing**"
      ],
      "metadata": {
        "id": "hLMV6kpPcYpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL TRAINING"
      ],
      "metadata": {
        "id": "M_u5p1L6cwqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import zscore\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score as r2, mean_squared_error as mse\n",
        "import math\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import confusion_matrix,classification_report"
      ],
      "metadata": {
        "id": "zrEZekzDdGih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 1** (excluding rows which has sales =0)\n"
      ],
      "metadata": {
        "id": "eQCZyQ_mdSKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "two dataset,first one having sales = '0' rows and another exculding it. We will both the data and find the best model."
      ],
      "metadata": {
        "id": "WQC0oDXYdxkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining dependent variable\n",
        "dependent_variables = 'Sales'\n",
        "\n",
        "# defining independent variable\n",
        "independent_variables = list(new_df.columns.drop(['Promo2SinceYear','Date','Sales']))"
      ],
      "metadata": {
        "id": "6voKr7XQd2wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List of Independent Variable\n",
        "independent_variables"
      ],
      "metadata": {
        "id": "85DR03FfeQzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data of independent variables\n",
        "X = new_df[independent_variables].values\n",
        "\n",
        "# Create the data of dependent variable\n",
        "y = new_df[dependent_variables].values"
      ],
      "metadata": {
        "id": "XzI_KWZ1Mr63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 1)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n"
      ],
      "metadata": {
        "id": "VPOhhW_DOJnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Supervised Machine Learning algorithms.\n",
        "1. Linear Regression (OLS)"
      ],
      "metadata": {
        "id": "gLEcb8DtKcaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg = LinearRegression().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "X5GCsiZsXbtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the Regression Score i.e R-squared value\n",
        "reg.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "KwPf-4xgXpuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the cofficient of different independent columns\n",
        "reg.coef_"
      ],
      "metadata": {
        "id": "AzXPVs6f0Ans"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the intercept of different indpendent columns\n",
        "reg.intercept_"
      ],
      "metadata": {
        "id": "-5zEhOyT0FQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting Dependent Variable With Test Dataset i.e 20%\n",
        "y_pred = reg.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "eMneQVlf0J1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original Test Dependent Value\n",
        "y_test"
      ],
      "metadata": {
        "id": "eXSz1qA00O3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting on Train Dataset\n",
        "y_pred_train = reg.predict(X_train)\n",
        "y_pred_train"
      ],
      "metadata": {
        "id": "sBlwkH9r0Tec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependent Variable With Train Dataset i.e 80 %\n",
        "y_train"
      ],
      "metadata": {
        "id": "c5aeMi0a0XlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate MSE & RMSE for Test Prediction\n",
        "MSE  = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)"
      ],
      "metadata": {
        "id": "7He1eVve0dYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Sm32kSb1Snh"
      },
      "outputs": [],
      "source": [
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R2 :\" ,r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LASSO**"
      ],
      "metadata": {
        "id": "NDuMh0hGVJin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "L1 = Lasso(alpha = 0.4, max_iter=10000,selection='cyclic', tol=0.0001,)"
      ],
      "metadata": {
        "id": "80KUPmjgV1tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L1.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "9JJQY6xHV7GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_lasso = L1.predict(X_test)"
      ],
      "metadata": {
        "id": "7DkjFX6D1SNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L1.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "m7PvPWo91WSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_scores = cross_val_score(L1, X, y, cv=10)\n",
        "mean_cv_score = cv_scores.mean()"
      ],
      "metadata": {
        "id": "MSKzDJeZ1cWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_scores"
      ],
      "metadata": {
        "id": "ybiF6VS_1g6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_cv_score"
      ],
      "metadata": {
        "id": "sMkdhXhG1lcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "# define the range of alpha values to test\n",
        "parameters = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5]}\n",
        "\n",
        "# perform grid search to find the best alpha value\n",
        "lasso_cv = GridSearchCV(L1, parameters, cv=5)\n",
        "lasso_cv.fit(X, y)"
      ],
      "metadata": {
        "id": "vQVYsfZL1oqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the best alpha value and corresponding score\n",
        "best_alpha_lasso = lasso_cv.best_params_['alpha']\n",
        "best_score_lasso= lasso_cv.best_score_"
      ],
      "metadata": {
        "id": "WcqAjEzn2aSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_alpha_lasso"
      ],
      "metadata": {
        "id": "VhCMpZsM2c3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_score_lasso"
      ],
      "metadata": {
        "id": "RaNohX_G2fM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(zip(y_test, y_pred_lasso), columns = ['actual', 'pred'])"
      ],
      "metadata": {
        "id": "lRZI6bEi2HZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge"
      ],
      "metadata": {
        "id": "7VRInutu3-hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L2 = Ridge(alpha = 0.5)"
      ],
      "metadata": {
        "id": "hMbJf8Zd4BMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L2.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "IUitVdVn6jyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L2.predict(X_test)"
      ],
      "metadata": {
        "id": "qERpTYeE6mrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L2.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "TfFd-Ld96pdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "\n",
        "ridge = Ridge(max_iter=10000, solver='auto')\n",
        "\n",
        "# define the range of alpha values to test\n",
        "parameters = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5]}\n",
        "\n",
        "# perform grid search to find the best alpha value\n",
        "ridge_cv = GridSearchCV(L2, parameters, cv=5)\n",
        "ridge_cv.fit(X, y)\n",
        "\n",
        "# extract the best alpha value and corresponding score\n",
        "best_alpha = ridge_cv.best_params_['alpha']\n",
        "best_score = ridge_cv.best_score_\n",
        "\n",
        "# perform cross-validation with the best alpha value\n",
        "ridge_best = Ridge(alpha=best_alpha, max_iter=10000, solver='auto')\n",
        "cv_scores = cross_val_score(ridge_best, X, y, cv=5)\n",
        "\n",
        "# find the maximum score and corresponding alpha value\n",
        "max_score = cv_scores.max()\n",
        "max_alpha = best_alpha\n",
        "\n",
        "print(\"Best alpha value: \", best_alpha)\n",
        "print(\"Best score: \", best_score)\n",
        "print(\"Maximum CV score: \", max_score)\n",
        "print(\"Corresponding alpha value: \", max_alpha)"
      ],
      "metadata": {
        "id": "KAZlROFT6tYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DECISION TREE**"
      ],
      "metadata": {
        "id": "a6VNB2cYWDbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_mean=final1[dependent_variables].mean()"
      ],
      "metadata": {
        "id": "KmP2NJ0i4Pet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_mean"
      ],
      "metadata": {
        "id": "_SiSr2WH4U9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_mean_new=new_df[dependent_variables].mean()"
      ],
      "metadata": {
        "id": "l3yRL0ao4YPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_mean_new"
      ],
      "metadata": {
        "id": "hYefQG1k4bKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree=DecisionTreeRegressor(max_depth=5)\n",
        "decision_tree.fit(X_train, y_train)\n",
        "y_pred_dt = decision_tree.predict(X_test)\n",
        "y_train_dt = decision_tree.predict(X_train)\n",
        "#print('dt_regressor R^2: ', r2(v_test,v_pred))\n",
        "MSE  = mean_squared_error(y_test, y_pred_dt)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "RMPSE=RMSE/sales_mean_new\n",
        "print(\"RMPSE :\",RMPSE)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred_dt)\n",
        "print(\"R2 :\" ,r2)"
      ],
      "metadata": {
        "id": "j4zi5-J14IuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final1"
      ],
      "metadata": {
        "id": "lWD367sy9svU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elastic Net"
      ],
      "metadata": {
        "id": "qri_eJx769dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# define the Elastic Net model\n",
        "elastic_net = ElasticNet(max_iter=10000)\n",
        "\n",
        "# define the range of alpha and l1_ratio values to test\n",
        "parameters = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5], 'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
        "\n",
        "# perform grid search to find the best alpha and l1_ratio values\n",
        "elastic_net_cv = GridSearchCV(elastic_net, parameters, cv=5)\n",
        "elastic_net_cv.fit(X_train, y_train)\n",
        "\n",
        "# extract the best alpha and l1_ratio values and corresponding score\n",
        "best_alpha = elastic_net_cv.best_params_['alpha']\n",
        "best_l1_ratio = elastic_net_cv.best_params_['l1_ratio']\n",
        "best_score = elastic_net_cv.best_score_\n",
        "\n",
        "# create an Elastic Net model with the best hyperparameters\n",
        "elastic_net_best = ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio, max_iter=10000)\n",
        "elastic_net_best.fit(X_train, y_train)\n",
        "\n",
        "# evaluate the model on the testing set\n",
        "test_score = elastic_net_best.score(X_test, y_test)\n",
        "\n",
        "print(\"Best alpha value: \", best_alpha)\n",
        "print(\"Best l1_ratio value: \", best_l1_ratio)\n",
        "print(\"Best score: \", best_score)\n",
        "print(\"Test score: \", test_score)"
      ],
      "metadata": {
        "id": "MGgl0dSS7Ebc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We saw that Sales column contains 172817 rows with 0 sale. So we created a new dataframe in which we removed 0 sales rows and tried to train our model. We used various algorithms and got accuracy score around 74%.\n",
        "\n",
        "We were also curious about the total dataset(including Sales = 0 rows). So we trained another model using various algorithms and we got accuracy near about 98% which is far better than previous model.\n",
        "\n",
        "So we came to conclusion that removing sales=0 rows actually removes lot of information from dataset as it has 172817 rows which is quite large and therefore we decided not to remove those values.We got our best rmpse score from Random Forest model,Graident boosting technique like adaboost ,Xgboost,we tried taking an optimum parameter so that our model doesnt overfit."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}