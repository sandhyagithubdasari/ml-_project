{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandhyagithubdasari/ml-_project/blob/main/Coronavirus_tweets_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Coronavirus  Tweet Sentiment Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This challenge asks you to build a classification model to predict the sentiment of COVID-19 tweets.The tweets have been pulled from Twitter and manual tagging has been done then.\n",
        "The names and usernames have been given codes to avoid any privacy concerns."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "COVID-19 originally known as Corona VIrus Disease of 2019, has been declared as a pandemic by World Health Organization (WHO) on 11th March 2020. Unprecedented pressures have mounted on each country to make compelling requisites for controlling the population by assessing the cases and properly utilizing available resources. The rapid number of exponential cases globally has become the apprehension of panic, fear and anxiety among people. The mental and physical health of the global population is found to be directly proportional to this pandemic disease. It is the need of the hour to implement different measures to safeguard the countries by demystifying the pertinent facts and information."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define Your Business Objective?**"
      ],
      "metadata": {
        "id": "PH-0ReGfmX4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "PhDvGCAqmjP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 20 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "nltk.download('all',quiet=True)\n",
        "from PIL import Image\n",
        "\n",
        "#Model libraries\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NYl4_nF7kqUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assigning variable\n",
        "df_orignal=pd.read_csv(\"/content/Coronavirus Tweets.csv\" ,encoding = 'latin-1')\n"
      ],
      "metadata": {
        "id": "CPvR9N-nkzL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#copying data to preserve orignal file\n",
        "df1=df_orignal.copy()"
      ],
      "metadata": {
        "id": "PbNO059evw1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET\n",
        "DRIVE LINK - https://drive.google.com/file/d/13BuZG1TR4U49s75x9TSqbvR85085xVNo/view?usp=sharing"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking info\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking Columns\n",
        "df1.columns"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For sentiment analysis we only want tweet and sentiment Features\n",
        "df=df1[['OriginalTweet','Sentiment']]"
      ],
      "metadata": {
        "id": "91kXcUagv6GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "#checking info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Names Of columns in our dataset\n",
        "df.columns"
      ],
      "metadata": {
        "id": "HXRBByj5nJ8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stastastical analysis of dataset\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "Dt_Sig0AqWJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check duplicate entries\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "JPigvhQowCCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking Unique values\n",
        "df.Sentiment.unique()"
      ],
      "metadata": {
        "id": "drLrRh76qYkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing"
      ],
      "metadata": {
        "id": "ePGnsWRywF2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to LowerCase :"
      ],
      "metadata": {
        "id": "tRbofSf7wHPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df[\"OriginalTweet\"] = df[\"OriginalTweet\"].str.lower()\n",
        "df['OriginalTweet']"
      ],
      "metadata": {
        "id": "TsOVffk6wJ9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Punctuations :"
      ],
      "metadata": {
        "id": "O0bxjnAowM3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['OriginalTweet'][0]"
      ],
      "metadata": {
        "id": "1M7ZXJ6OtzNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['OriginalTweet'] = df['OriginalTweet'].str.replace('http\\S+|www.\\S+', '', case=False)"
      ],
      "metadata": {
        "id": "nbmLT5ykwURZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def remove_punctuations(text):\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text"
      ],
      "metadata": {
        "id": "bs8LD3mmwXHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"clean_tweets\"] = df['OriginalTweet'].apply(remove_punctuations)\n"
      ],
      "metadata": {
        "id": "e2ZgTdkkwY7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_tweets']"
      ],
      "metadata": {
        "id": "n4ss9Imgwdbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_tweets'] = df['clean_tweets'].str.replace(\"[^a-zA-Z#//]\",\" \")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LXDsoU9bwfu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_tweets'][0]"
      ],
      "metadata": {
        "id": "Y0zHvMrQwh7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Stop Words :"
      ],
      "metadata": {
        "id": "sc1bQlpAwkJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Stop-words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')"
      ],
      "metadata": {
        "id": "-i0mYGjtwmov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to remove stopwords and tokenize\n",
        "def remove_stopwords(text):\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "    return (text)"
      ],
      "metadata": {
        "id": "9s1q9-6qwoh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_tweets']= df['clean_tweets'].apply(lambda x: remove_stopwords(x))"
      ],
      "metadata": {
        "id": "GhuLjfJ_wqsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.clean_tweets[6]"
      ],
      "metadata": {
        "id": "kuLxRyT_wsv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W-NDSx8WwvSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "N96f5QtIw4bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for stemming\n",
        "def stemming(text):\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    return (\" \".join(text))"
      ],
      "metadata": {
        "id": "tHlUnkuLw7c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['stemmed'] = df['clean_tweets'].apply(lambda x: stemming(x))"
      ],
      "metadata": {
        "id": "xHtJRFv2xEBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#result\n",
        "df.stemmed.head()"
      ],
      "metadata": {
        "id": "HOP2TZqQxHxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation :\n",
        "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma."
      ],
      "metadata": {
        "id": "hgznwgArxKL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatizing\n"
      ],
      "metadata": {
        "id": "3PpfqI8Gxn3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizing\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "df['lemmed'] = df['clean_tweets'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
      ],
      "metadata": {
        "id": "WRKubVCuxute"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "pCERrOtLxxW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation :\n",
        "Lemmatization is the grouping together of different forms of the same word. In search queries, lemmatization allows end users to query any version of a base word and get relevant results."
      ],
      "metadata": {
        "id": "J6m8otP_xzL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tweet Count For Each Sentiment"
      ],
      "metadata": {
        "id": "w3K90HuKy0sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_count = df['Sentiment'].value_counts().reset_index()\n",
        "sentiment_count.columns = ['Sentiment','count']\n",
        "sentiment_count"
      ],
      "metadata": {
        "id": "P_oZzo5ly1uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "ax = sns.barplot(x=\"Sentiment\", y='count', data=sentiment_count)\n",
        "ax.set_title(\"Proportion of Sentiment\", fontsize=20)\n",
        "ax.set_xlabel(\"Sentiment\", fontsize=20)\n",
        "ax.set_ylabel('count', fontsize=20)"
      ],
      "metadata": {
        "id": "Nb3DpusPy6a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation :\n",
        "Number of \"Positive\" sentiments are higher than all other sentiments"
      ],
      "metadata": {
        "id": "Z-oH2HECy821"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replacing values\n",
        "replace_values = {\"Sentiment\":{'Extremely Negative':'Negative', 'Extremely Positive':'Positive'}}\n",
        "df = df.replace(replace_values)"
      ],
      "metadata": {
        "id": "BGO19_dny_49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_count1 = df['Sentiment'].value_counts().reset_index()\n",
        "sentiment_count1.columns = ['Sentiment','count']\n",
        "sentiment_count1"
      ],
      "metadata": {
        "id": "GJH5kZ41zBt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the piechart for Sentiments distribution\n",
        "sentiment_count1 = df['Sentiment'].value_counts().to_list()\n",
        "labels=['Positive','Negative','Netural']\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.pie(x=sentiment_count1,explode=[0.04,0.04,0.1],shadow= True,labels=labels,autopct=\"%.2f%%\",radius=1.1)\n",
        "plt.title(\"Proportion Of Sentiments\", fontsize=20)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bNZUxP_0zD3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obeservation :\n",
        "We combined Extremly Positive and Negative sentiments to positive and negative sentiments respectively. As we can observe on the Pie-Plot, The total number of \"Positive\" sentiments are still high after combining."
      ],
      "metadata": {
        "id": "jpKVv_Y3zGlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['temp_list'] = df['clean_tweets'].apply(lambda x:str(x).split())"
      ],
      "metadata": {
        "id": "HRUL-5IlzIqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "top = Counter([item for sublist in df['temp_list'] for item in sublist])\n",
        "temp = pd.DataFrame(top.most_common(20))\n",
        "temp.columns = ['Common_words','count']\n",
        "temp.style.background_gradient(cmap='Reds')"
      ],
      "metadata": {
        "id": "o9bjmlJ5zLSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word cloud"
      ],
      "metadata": {
        "id": "CEyE12e_zQVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(2)"
      ],
      "metadata": {
        "id": "t2eq6z4vz3PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seperating the sentiments for word cloud\n",
        "neutral = pd.DataFrame(df[['stemmed','lemmed']] [df['Sentiment'] == 'Neutral'])\n",
        "positive = pd.DataFrame(df[['stemmed','lemmed']]  [df['Sentiment'] == 'Positive'])\n",
        "negative = pd.DataFrame(df[['stemmed','lemmed']]  [df['Sentiment'] == 'Negative'])"
      ],
      "metadata": {
        "id": "CJrywCF5zRQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seperating the sentiments for word cloud\n",
        "neutral = pd.DataFrame(df[['stemmed','lemmed']] [df['Sentiment'] == 'Neutral'])\n",
        "positive = pd.DataFrame(df[['stemmed','lemmed']]  [df['Sentiment'] == 'Positive'])\n",
        "negative = pd.DataFrame(df[['stemmed','lemmed']]  [df['Sentiment'] == 'Negative'])"
      ],
      "metadata": {
        "id": "bnQYawa0zVwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Test Split"
      ],
      "metadata": {
        "id": "up9jiZZy16ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assigning dependent and independent features\n",
        "X= df['lemmed']\n",
        "y=df['Sentiment']"
      ],
      "metadata": {
        "id": "4kQBuWWx19G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Train test split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,stratify=y,random_state=10)"
      ],
      "metadata": {
        "id": "vfC2xwkz1_O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking shape of splitted data\n",
        "print(X_train.shape)\n",
        "y_test.shape"
      ],
      "metadata": {
        "id": "r3Cy60Sm2A9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking splitted data\n",
        "print(X_train.head())\n",
        "y_train.head()"
      ],
      "metadata": {
        "id": "DPONvE_H5vDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Count Vectorization (Bag of words) and TF/IDF Vecorization"
      ],
      "metadata": {
        "id": "vT46yLqQ2FTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
      ],
      "metadata": {
        "id": "MI_MY0mf5-7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of words\n",
        "cv=CountVectorizer(binary=False,max_df=1.0,min_df=5,ngram_range=(1,2))\n",
        "cv_X_train=cv.fit_transform(X_train.astype(str).str.strip())"
      ],
      "metadata": {
        "id": "DkoVl1rF6Dla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "tv=TfidfVectorizer(use_idf=True,max_df=1.0,min_df=5,ngram_range=(1,2),sublinear_tf=True)\n",
        "tv_X_train=tv.fit_transform(X_train.astype(str).str.strip())"
      ],
      "metadata": {
        "id": "Pa3zjAFi6FBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tv_X_train.shape"
      ],
      "metadata": {
        "id": "a3cXS3k86GuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_X_test=cv.transform(X_test.astype(str).str.strip())\n",
        "tv_X_test=tv.transform(X_test.astype(str).str.strip())"
      ],
      "metadata": {
        "id": "pS0QmqFT6IPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Logistic Regression Count Vectoriser Method with Gridsearch CV"
      ],
      "metadata": {
        "id": "foIFgLwN2iNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initalizing the model\n",
        "lr_cv = LogisticRegression()\n",
        "parameters = dict(penalty=['l1', 'l2'],C=[100, 10, 1.0, 0.1, 0.01])\n",
        "\n",
        "#Hyperparameter tuning by GridserchCV\n",
        "logreg_Gcv=GridSearchCV(lr_cv,parameters,cv=15)\n",
        "\n",
        "#fitting the data to model\n",
        "logreg_Gcv.fit(cv_X_train,y_train)"
      ],
      "metadata": {
        "id": "nwtCN-ea2lFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicted values\n",
        "pred_lr_cv = logreg_Gcv.predict(cv_X_test)"
      ],
      "metadata": {
        "id": "WlphhPg02qij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lr_cv"
      ],
      "metadata": {
        "id": "VCVC5hPM2unr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy\n",
        "accuracy_lr_cv = accuracy_score(y_test,pred_lr_cv)\n",
        "print(\"Accuracy :\",(accuracy_lr_cv))"
      ],
      "metadata": {
        "id": "X88k1-1h2web"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['neutral','positive','negative']\n",
        "print(classification_report(y_test,pred_lr_cv))"
      ],
      "metadata": {
        "id": "bbJIDivW2yM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Confussion matrix\n",
        "cf1= (confusion_matrix(y_test,pred_lr_cv))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf1, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Logistic Regression with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "7qZdDRk520Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation :\n",
        "This model gives an accuracy score of 78.28% which implies that our model is performing well."
      ],
      "metadata": {
        "id": "3TV6fKCh3CPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Decision Tree Classifier with CV"
      ],
      "metadata": {
        "id": "AG64KFlZ3I-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing model\n",
        "dt_cv=DecisionTreeClassifier()\n",
        "\n",
        "#fitting the data to model\n",
        "dt_cv.fit(cv_X_train,y_train)\n",
        "\n",
        "#predicted values\n",
        "pred_dt_cv=dt_cv.predict(cv_X_test)"
      ],
      "metadata": {
        "id": "B6thgEcV3Lct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_dt_cv"
      ],
      "metadata": {
        "id": "mPTI2JZf3Pec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "cv_score_dt_cv= cross_val_score(dt_cv,cv_X_train,y_train, cv=5)\n",
        "print(\"Accuracy: {}\" .format(np.mean(cv_score_dt_cv)))"
      ],
      "metadata": {
        "id": "W5V1qZNN3RJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['Neutral','Positive','Negative']\n",
        "print(classification_report(y_test,pred_dt_cv))"
      ],
      "metadata": {
        "id": "Jme2qbIp3Sv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Confussion matrix\n",
        "cf2= (confusion_matrix(y_test,pred_dt_cv))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf2, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Decision tree with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "lWT3wIN53Uw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation :\n",
        "This model gives an accuracy score of 69.04% which implies that our model is performing well."
      ],
      "metadata": {
        "id": "XmmlRyDq3Wqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " K-NN Count Vectorizer with GridsearchCV"
      ],
      "metadata": {
        "id": "KX2lhzu43efc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing model\n",
        "knn = KNeighborsClassifier()\n",
        "param = {'n_neighbors': [1,2,3,4,5,6,7,8]}\n",
        "knn_cv = GridSearchCV(estimator=knn,param_grid=param)\n",
        "\n",
        "#fitting the data to model\n",
        "knn_cv.fit(cv_X_train, y_train)"
      ],
      "metadata": {
        "id": "ua2FxA62412L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicted values\n",
        "pred_knn_cv = knn_cv.predict(cv_X_test)\n"
      ],
      "metadata": {
        "id": "36lE_DAf9yPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_knn_cv"
      ],
      "metadata": {
        "id": "MpwO5iXX90rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "accuracy_KNN = accuracy_score(y_test,pred_knn_cv)\n",
        "print(\"Accuracy :\",(accuracy_KNN))"
      ],
      "metadata": {
        "id": "srpYoL8x94sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['Neutral','Positive','Negative']\n",
        "print(classification_report(y_test,pred_knn_cv))"
      ],
      "metadata": {
        "id": "W_3-m8AA96iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Confussion matrix\n",
        "cf4= (confusion_matrix(y_test,pred_knn_cv))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf4, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (KNN with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "YuI4p0yu98IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Observation :\n",
        "The model is giving an accuracy score of 41% which implies that our model is underperforming."
      ],
      "metadata": {
        "id": "PFOYL-d4991C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RSXmUtlH-EHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " SVM with CV"
      ],
      "metadata": {
        "id": "qFrt4Jvw-ENh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing model\n",
        "svm_cv = SVC()\n",
        "\n",
        "#fitting the data to model\n",
        "svm_cv.fit(cv_X_train,y_train)\n",
        "\n",
        "#prediction\n",
        "pred_svm_cv = svm_cv.predict(cv_X_test)"
      ],
      "metadata": {
        "id": "DQZaF0JX-FCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Confussion matrix\n",
        "cf4= (confusion_matrix(y_test,pred_knn_cv))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf4, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (KNN with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "-1AW-p-v5gCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation :\n",
        "The model is giving an accuracy score of 41% which implies that our model is underperforming."
      ],
      "metadata": {
        "id": "GqzIIgO35idM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision tree classifier with TF/IDF"
      ],
      "metadata": {
        "id": "JlP6UdQhMD0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing model\n",
        "dt_tv=DecisionTreeClassifier()\n",
        "\n",
        "#fitting the data to model\n",
        "dt_tv.fit(tv_X_train,y_train)\n",
        "\n",
        "#prediction\n",
        "pred_dt_tv=dt_tv.predict(tv_X_test)"
      ],
      "metadata": {
        "id": "h7Ery5sVMErf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_dt_tv"
      ],
      "metadata": {
        "id": "l0HnkufIMHpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "cv_score_dt_tv= cross_val_score(dt_tv,tv_X_train,y_train, cv=5)\n",
        "print(\"Accuracy: {}\" .format(np.mean(cv_score_dt_tv)))"
      ],
      "metadata": {
        "id": "YoLrd49XMKox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['Neutral','Positive','Negative']\n",
        "print(classification_report(y_test,pred_dt_tv))"
      ],
      "metadata": {
        "id": "H0t_fjzzMNJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Confussion matrix\n",
        "cf2a= (confusion_matrix(y_test,pred_dt_tv))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf2a, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Decision Tree with TF/IDF)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "Z6C9LATKMQKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation :\n",
        "This model gives an accuracy score of 60% which implies that our model is performing well."
      ],
      "metadata": {
        "id": "z8AhPJNQMS9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusions:"
      ],
      "metadata": {
        "id": "tORKRlCO7wkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. We applied 5 different machine learing models namely, Logistic Regression with Grid Search CV, Desision Tree Classifier, XG Boost, KNN, SVM Classifier for both Count Vector And TF IDF Vectorisation techniques.\n",
        "2. We conclude that the machine is generating best results for Logistic Regression with Grid Search CV model with and Accuracy score of 78.28% and 77.43% respectively for Count vector and TF/idf Vector, followed by SVM\n",
        "3. Also, we observed that no overfitting is seen for the data, and we can deploy this model.\n",
        "4. The sentiments of future tweets can be easily predicted using this model."
      ],
      "metadata": {
        "id": "k4GLzHVE7vhg"
      }
    }
  ]
}